{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2200a910",
   "metadata": {},
   "source": [
    "# ğŸ¦ Extract Tweets from Data Science Club\n",
    "\n",
    "This notebook uses **RapidAPI** to fetch the latest tweets from the Data Science Club account.\n",
    "\n",
    "## Features:\n",
    "- âœ… Free (50 requests/day)\n",
    "- âœ… Reliable and working\n",
    "- âœ… Easy to use\n",
    "\n",
    "## Requirements:\n",
    "1. Account on [RapidAPI](https://rapidapi.com/)\n",
    "2. Subscription to [Twitter241 API](https://rapidapi.com/davethebeast/api/twitter241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f949ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d25058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# ==========================================\n",
    "# ğŸ”‘ RapidAPI Key Setup\n",
    "# ==========================================\n",
    "# Get your Key from: https://rapidapi.com/davethebeast/api/twitter241\n",
    "\n",
    "RAPIDAPI_KEY = \"b7b8e17920msh995b55fdc65f61ap1f1417jsn33f3dd92a4b7\"  # Put your API Key here\n",
    "\n",
    "# Data Science Club username\n",
    "DSC_USERNAME = \"DSC_KAU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ad248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_rapidapi(username: str, rapidapi_key: str, count: int = 5):\n",
    "    \"\"\"\n",
    "    ğŸ”‘ Fetch tweets using RapidAPI (Free - 50 requests/day)\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” Fetching tweets for @{username} from RapidAPI...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    user_url = \"https://twitter241.p.rapidapi.com/user\"\n",
    "    tweets_url = \"https://twitter241.p.rapidapi.com/user-tweets\"\n",
    "    \n",
    "    headers = {\n",
    "        \"x-rapidapi-key\": rapidapi_key,\n",
    "        \"x-rapidapi-host\": \"twitter241.p.rapidapi.com\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get user information\n",
    "        print(\"ğŸ”„ Searching for account...\")\n",
    "        user_response = requests.get(\n",
    "            user_url, \n",
    "            headers=headers, \n",
    "            params={\"username\": username}\n",
    "        )\n",
    "        \n",
    "        if user_response.status_code == 403:\n",
    "            print(\"âŒ Invalid or expired API Key\")\n",
    "            return None\n",
    "            \n",
    "        if user_response.status_code != 200:\n",
    "            print(f\"âŒ Error: {user_response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        user_data = user_response.json()\n",
    "        \n",
    "        # Extract user_id\n",
    "        user_id = None\n",
    "        if 'result' in user_data and 'data' in user_data['result']:\n",
    "            user_id = user_data['result']['data']['user']['result']['rest_id']\n",
    "        elif 'data' in user_data:\n",
    "            user_id = user_data['data']['user']['result']['rest_id']\n",
    "            \n",
    "        if not user_id:\n",
    "            print(\"âŒ Account not found\")\n",
    "            print(f\"Response: {user_data}\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"âœ… Account found! User ID: {user_id}\")\n",
    "        \n",
    "        # Fetch tweets\n",
    "        print(\"ğŸ”„ Fetching tweets...\")\n",
    "        tweets_response = requests.get(\n",
    "            tweets_url,\n",
    "            headers=headers,\n",
    "            params={\"user\": user_id, \"count\": str(count * 2)}\n",
    "        )\n",
    "        \n",
    "        if tweets_response.status_code != 200:\n",
    "            print(f\"âŒ Error fetching tweets: {tweets_response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        tweets_data = tweets_response.json()\n",
    "        \n",
    "        # Try to extract tweets using different methods\n",
    "        tweets_list = []\n",
    "        \n",
    "        # Method 1: Old structure\n",
    "        instructions = tweets_data.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "        \n",
    "        # Method 2: New structure\n",
    "        if not instructions:\n",
    "            instructions = tweets_data.get('data', {}).get('user', {}).get('result', {}).get('timeline_v2', {}).get('timeline', {}).get('instructions', [])\n",
    "        \n",
    "        # Search for entries\n",
    "        entries = []\n",
    "        for instruction in instructions:\n",
    "            if 'entries' in instruction:\n",
    "                entries = instruction['entries']\n",
    "                break\n",
    "            elif instruction.get('type') == 'TimelineAddEntries':\n",
    "                entries = instruction.get('entries', [])\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nğŸ¦ Last {count} tweets for @{username}:\\n\")\n",
    "        \n",
    "        tweet_count = 0\n",
    "        for entry in entries:\n",
    "            if tweet_count >= count:\n",
    "                break\n",
    "            \n",
    "            entry_id = entry.get('entryId', '')\n",
    "            \n",
    "            # Skip non-tweets\n",
    "            if 'tweet' not in entry_id and 'Tweet' not in entry_id:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Try to extract tweet\n",
    "                content = entry.get('content', {})\n",
    "                item_content = content.get('itemContent', {})\n",
    "                tweet_results = item_content.get('tweet_results', {})\n",
    "                result = tweet_results.get('result', {})\n",
    "                \n",
    "                # Handle TweetWithVisibilityResults\n",
    "                if result.get('__typename') == 'TweetWithVisibilityResults':\n",
    "                    result = result.get('tweet', {})\n",
    "                \n",
    "                legacy = result.get('legacy', {})\n",
    "                \n",
    "                if not legacy:\n",
    "                    continue\n",
    "                \n",
    "                text = legacy.get('full_text', 'N/A')\n",
    "                likes = legacy.get('favorite_count', 0)\n",
    "                retweets = legacy.get('retweet_count', 0)\n",
    "                created_at = legacy.get('created_at', 'N/A')\n",
    "                \n",
    "                tweet_data = {\n",
    "                    'text': text,\n",
    "                    'likes': likes,\n",
    "                    'retweets': retweets,\n",
    "                    'date': created_at\n",
    "                }\n",
    "                tweets_list.append(tweet_data)\n",
    "                \n",
    "                tweet_count += 1\n",
    "                print(f\"ğŸ“ Tweet {tweet_count}:\")\n",
    "                print(f\"   ğŸ“… Date: {created_at}\")\n",
    "                print(f\"   ğŸ’¬ Text: {text[:100]}...\" if len(text) > 100 else f\"   ğŸ’¬ Text: {text}\")\n",
    "                print(f\"   â¤ï¸ Likes: {likes}\")\n",
    "                print(f\"   ğŸ”„ Retweets: {retweets}\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if not tweets_list:\n",
    "            print(\"âŒ No tweets found\")\n",
    "            print(\"\\nğŸ“‹ Debug - Response structure:\")\n",
    "            print(f\"Keys: {tweets_data.keys()}\")\n",
    "            if 'result' in tweets_data:\n",
    "                print(f\"Result keys: {tweets_data['result'].keys() if isinstance(tweets_data['result'], dict) else 'N/A'}\")\n",
    "            \n",
    "        return tweets_list\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ ØªØºØ±ÙŠØ¯Ø§Øª @DSC_KAU Ù…Ù† RapidAPI...\n",
      "==================================================\n",
      "ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø­Ø³Ø§Ø¨...\n",
      "âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø³Ø§Ø¨! User ID: 1608405795707752448\n",
      "ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª...\n",
      "\n",
      "ğŸ¦ Ø¢Ø®Ø± 5 ØªØºØ±ÙŠØ¯Ø§Øª Ù„Ù€ @DSC_KAU:\n",
      "\n",
      "ğŸ“ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 1:\n",
      "   ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: Tue Dec 02 18:49:33 +0000 2025\n",
      "   ğŸ’¬ Ø§Ù„Ù†Øµ: ÙŠØ³Ø± Ù†Ø§Ø¯ÙŠ Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† Ø¹Ù† Ø§Ù„ÙØ±Ù‚ Ø§Ù„ÙØ§Ø¦Ø²Ø© ÙÙŠ Ù…Ø³Ø§Ø¨Ù‚Ø© Ø£ÙØ¶Ù„ Ù…Ù‚ØªØ±Ø­âœ¨\n",
      "\n",
      "ğŸ¥‡ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„: Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø¹ÙŠØ©\n",
      "ğŸ¥ˆ...\n",
      "   â¤ï¸ Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª: 23\n",
      "   ğŸ”„ Ø±ÙŠØªÙˆÙŠØª: 2\n",
      "----------------------------------------\n",
      "ğŸ“ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 2:\n",
      "   ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: Wed Nov 26 17:35:41 +0000 2025\n",
      "   ğŸ’¬ Ø§Ù„Ù†Øµ: Ø§Ù„ØªØ®Ø·ÙŠØ·ğŸ“! \n",
      "Ø­Ù„Ù‚Ø© Ø§Ù„ÙˆØµÙ„ Ø¨ÙŠÙ† Ø§Ù„ÙÙƒØ±Ø© ÙˆØ§Ù„ØªÙ†ÙÙŠØ° ğŸš€\n",
      "\n",
      "Ù†Ø§Ø¯ÙŠ Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙ‚Ø¯Ù… Ù„ÙƒÙ… ÙˆØ±Ø´Ø© Ø¨Ø¹Ù†ÙˆØ§Ù† :\n",
      "â€œØ§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„ÙØ¹Ù‘...\n",
      "   â¤ï¸ Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª: 23\n",
      "   ğŸ”„ Ø±ÙŠØªÙˆÙŠØª: 3\n",
      "----------------------------------------\n",
      "ğŸ“ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 3:\n",
      "   ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: Wed Nov 26 15:16:46 +0000 2025\n",
      "   ğŸ’¬ Ø§Ù„Ù†Øµ: Ø¬Ø§Ù‡Ø²ÙŠÙ† Ù„Ù…Ø³Ø§Ø¨Ù‚Ø© ÙŠÙ‚Ø¯Ù…Ù‡Ø§ Ù†Ø§Ø¯ÙŠ Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªğŸ”¥ØŸ\n",
      "\n",
      "Ù†Ø¹Ù„Ù† Ù„ÙƒÙ… Ø¹Ù† ÙØªØ­ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Ù…Ø³Ø§Ø¨Ù‚Ø©:\n",
      "\n",
      "â€œØ£ÙØ¶Ù„ Ù…Ù‚ØªØ±Ø­â€ ğŸ¯âœ¨\n",
      "\n",
      "Ù‚Ø¯Ù‘Ù…...\n",
      "   â¤ï¸ Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª: 40\n",
      "   ğŸ”„ Ø±ÙŠØªÙˆÙŠØª: 8\n",
      "----------------------------------------\n",
      "ğŸ“ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 4:\n",
      "   ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: Mon Nov 10 18:20:17 +0000 2025\n",
      "   ğŸ’¬ Ø§Ù„Ù†Øµ: Ù„Ø£Ù† Ø§Ù„Ø¹ÙŠÙ† ØªÙÙ‡Ù… Ø£Ø³Ø±Ø¹ Ù…Ù† Ø§Ù„ÙƒÙ„Ø§Ù… ğŸ‘€\n",
      "\n",
      "Ø¬Ø§ÙŠÙŠÙ†ÙƒÙ… ÙÙŠ ÙˆØ±Ø´Ø© Ø¹Ù…Ù„ Ø¨Ø¹Ù†ÙˆØ§Ù†:\n",
      " Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Seaborn &amp; Ma...\n",
      "   â¤ï¸ Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª: 34\n",
      "   ğŸ”„ Ø±ÙŠØªÙˆÙŠØª: 9\n",
      "----------------------------------------\n",
      "ğŸ“ Ø§Ù„ØªØºØ±ÙŠØ¯Ø© 5:\n",
      "   ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: Mon Nov 10 11:39:25 +0000 2025\n",
      "   ğŸ’¬ Ø§Ù„Ù†Øµ: Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø±Ø¤Ù‰ Ù…Ø¹ ÙˆØ±Ø´Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§ÙÙŠ ğŸ”ğŸ“ˆ\n",
      "\n",
      "Ø±Ø­Ù„Ø© Ø´ÙŠÙ‘Ù‚Ø© Ø®Ø§Ø¶Ù‡Ø§ Ø§Ù„Ø·Ù„Ø§Ø¨ ÙˆØ§Ù„Ø·Ø§Ù„Ø¨Ø§Øª Ù„Ø§ÙƒØªØ´Ø§...\n",
      "   â¤ï¸ Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª: 38\n",
      "   ğŸ”„ Ø±ÙŠØªÙˆÙŠØª: 4\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ğŸš€ Run the code\n",
    "# ==========================================\n",
    "\n",
    "tweets = get_tweets_rapidapi(DSC_USERNAME, RAPIDAPI_KEY, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9f08d",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Save Tweets to JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceed9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø­ÙØ¸ 5 ØªØºØ±ÙŠØ¯Ø§Øª ÙÙŠ: dsc_tweets.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_tweets_to_json(tweets, filename=\"dsc_tweets.json\"):\n",
    "    \"\"\"Save tweets to a JSON file\"\"\"\n",
    "    if not tweets:\n",
    "        print(\"âŒ No tweets to save\")\n",
    "        return\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tweets, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Saved {len(tweets)} tweets to: {filename}\")\n",
    "\n",
    "# Save the tweets\n",
    "if tweets:\n",
    "    save_tweets_to_json(tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
