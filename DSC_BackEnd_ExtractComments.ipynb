{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d66702",
   "metadata": {},
   "source": [
    "# üí¨ Extract Comments on Data Science Club Tweets\n",
    "\n",
    "This notebook uses **RapidAPI** to fetch comments/replies on the latest tweets from the Data Science Club account.\n",
    "\n",
    "## Features:\n",
    "- ‚úÖ Free (50 requests/day)\n",
    "- ‚úÖ Extracts replies on tweets\n",
    "- ‚úÖ Saves comments to JSON\n",
    "\n",
    "## Requirements:\n",
    "1. Account on [RapidAPI](https://rapidapi.com/)\n",
    "2. Subscription to [Twitter241 API](https://rapidapi.com/davethebeast/api/twitter241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a324e7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moaya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install requests python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25d0307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API Key loaded successfully\n",
      "‚úÖ Username: @DSC_KAU\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==========================================\n",
    "# üîë Load environment variables from .env\n",
    "# ==========================================\n",
    "load_dotenv()\n",
    "\n",
    "RAPIDAPI_KEY = os.getenv(\"RAPIDAPI_KEY\")\n",
    "DSC_USERNAME = os.getenv(\"DSC_USERNAME\", \"DSC_KAU\")\n",
    "\n",
    "# API Headers\n",
    "HEADERS = {\n",
    "    \"x-rapidapi-key\": RAPIDAPI_KEY,\n",
    "    \"x-rapidapi-host\": \"twitter241.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "# Validate\n",
    "if not RAPIDAPI_KEY:\n",
    "    print(\"‚ùå Error: RAPIDAPI_KEY not found in .env file!\")\n",
    "else:\n",
    "    print(f\"‚úÖ API Key loaded successfully\")\n",
    "    print(f\"‚úÖ Username: @{DSC_USERNAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa8e851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_id(username: str) -> str:\n",
    "    \"\"\"\n",
    "    Get user ID from username\n",
    "    \"\"\"\n",
    "    user_url = \"https://twitter241.p.rapidapi.com/user\"\n",
    "    \n",
    "    response = requests.get(\n",
    "        user_url,\n",
    "        headers=HEADERS,\n",
    "        params={\"username\": username}\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    user_data = response.json()\n",
    "    \n",
    "    # Extract user_id\n",
    "    if 'result' in user_data and 'data' in user_data['result']:\n",
    "        return user_data['result']['data']['user']['result']['rest_id']\n",
    "    elif 'data' in user_data:\n",
    "        return user_data['data']['user']['result']['rest_id']\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12f93bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_tweets(user_id: str, count: int = 5) -> list:\n",
    "    \"\"\"\n",
    "    Get the latest tweets from a user\n",
    "    \"\"\"\n",
    "    tweets_url = \"https://twitter241.p.rapidapi.com/user-tweets\"\n",
    "    \n",
    "    response = requests.get(\n",
    "        tweets_url,\n",
    "        headers=HEADERS,\n",
    "        params={\"user\": user_id, \"count\": str(count * 2)}\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Error fetching tweets: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    tweets_data = response.json()\n",
    "    \n",
    "    # Extract tweets\n",
    "    tweets_list = []\n",
    "    \n",
    "    # Get instructions\n",
    "    instructions = tweets_data.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "    if not instructions:\n",
    "        instructions = tweets_data.get('data', {}).get('user', {}).get('result', {}).get('timeline_v2', {}).get('timeline', {}).get('instructions', [])\n",
    "    \n",
    "    # Find entries\n",
    "    entries = []\n",
    "    for instruction in instructions:\n",
    "        if 'entries' in instruction:\n",
    "            entries = instruction['entries']\n",
    "            break\n",
    "        elif instruction.get('type') == 'TimelineAddEntries':\n",
    "            entries = instruction.get('entries', [])\n",
    "            break\n",
    "    \n",
    "    tweet_count = 0\n",
    "    for entry in entries:\n",
    "        if tweet_count >= count:\n",
    "            break\n",
    "        \n",
    "        entry_id = entry.get('entryId', '')\n",
    "        \n",
    "        if 'tweet' not in entry_id and 'Tweet' not in entry_id:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            content = entry.get('content', {})\n",
    "            item_content = content.get('itemContent', {})\n",
    "            tweet_results = item_content.get('tweet_results', {})\n",
    "            result = tweet_results.get('result', {})\n",
    "            \n",
    "            if result.get('__typename') == 'TweetWithVisibilityResults':\n",
    "                result = result.get('tweet', {})\n",
    "            \n",
    "            legacy = result.get('legacy', {})\n",
    "            \n",
    "            if not legacy:\n",
    "                continue\n",
    "            \n",
    "            tweet_id = legacy.get('id_str', result.get('rest_id', ''))\n",
    "            text = legacy.get('full_text', 'N/A')\n",
    "            reply_count = legacy.get('reply_count', 0)\n",
    "            \n",
    "            tweets_list.append({\n",
    "                'id': tweet_id,\n",
    "                'text': text,\n",
    "                'reply_count': reply_count\n",
    "            })\n",
    "            \n",
    "            tweet_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81b74165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_comments(tweet_id: str, max_comments: int = 20) -> list:\n",
    "    \"\"\"\n",
    "    Get comments/replies on a specific tweet using tweet detail endpoint\n",
    "    \"\"\"\n",
    "    detail_url = \"https://twitter241.p.rapidapi.com/tweet\"\n",
    "    \n",
    "    response = requests.get(\n",
    "        detail_url,\n",
    "        headers=HEADERS,\n",
    "        params={\"pid\": tweet_id}\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"   ‚ö†Ô∏è Error {response.status_code} for tweet {tweet_id}\")\n",
    "        return []\n",
    "    \n",
    "    tweet_data = response.json()\n",
    "    comments = []\n",
    "    \n",
    "    try:\n",
    "        # Navigate to the correct structure\n",
    "        conversation = tweet_data.get('data', {}).get('threaded_conversation_with_injections_v2', {})\n",
    "        instructions = conversation.get('instructions', [])\n",
    "        \n",
    "        entries = []\n",
    "        for instruction in instructions:\n",
    "            if instruction.get('type') == 'TimelineAddEntries':\n",
    "                entries = instruction.get('entries', [])\n",
    "                break\n",
    "        \n",
    "        for entry in entries:\n",
    "            entry_id = entry.get('entryId', '')\n",
    "            \n",
    "            # Look for conversation thread entries (these are replies)\n",
    "            if 'conversationthread' in entry_id.lower():\n",
    "                content = entry.get('content', {})\n",
    "                items = content.get('items', [])\n",
    "                \n",
    "                for item in items:\n",
    "                    try:\n",
    "                        item_content = item.get('item', {}).get('itemContent', {})\n",
    "                        tweet_results = item_content.get('tweet_results', {})\n",
    "                        result = tweet_results.get('result', {})\n",
    "                        \n",
    "                        if result.get('__typename') == 'TweetWithVisibilityResults':\n",
    "                            result = result.get('tweet', {})\n",
    "                        \n",
    "                        legacy = result.get('legacy', {})\n",
    "                        core = result.get('core', {}).get('user_results', {}).get('result', {})\n",
    "                        user_legacy = core.get('legacy', {})\n",
    "                        \n",
    "                        # Skip the original tweet\n",
    "                        if legacy and legacy.get('id_str') != tweet_id:\n",
    "                            comment_text = legacy.get('full_text', '')\n",
    "                            if comment_text:\n",
    "                                comments.append({\n",
    "                                    'comment_id': legacy.get('id_str', ''),\n",
    "                                    'text': comment_text,\n",
    "                                    'username': user_legacy.get('screen_name', 'Unknown'),\n",
    "                                    'name': user_legacy.get('name', 'Unknown'),\n",
    "                                    'likes': legacy.get('favorite_count', 0),\n",
    "                                    'date': legacy.get('created_at', '')\n",
    "                                })\n",
    "                                \n",
    "                                if len(comments) >= max_comments:\n",
    "                                    break\n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error parsing: {e}\")\n",
    "    \n",
    "    return comments\n",
    "\n",
    "\n",
    "def get_all_replies_via_search(username: str, tweet_ids: list) -> dict:\n",
    "    \"\"\"\n",
    "    Get all replies to a user using search, then filter by tweet IDs\n",
    "    \"\"\"\n",
    "    print(f\"üîç Searching for all replies to @{username}...\")\n",
    "    \n",
    "    search_url = \"https://twitter241.p.rapidapi.com/search-v2\"\n",
    "    \n",
    "    response = requests.get(\n",
    "        search_url,\n",
    "        headers=HEADERS,\n",
    "        params={\"query\": f\"to:{username}\", \"count\": \"100\", \"type\": \"Latest\"}\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"   ‚ö†Ô∏è Search failed: {response.status_code}\")\n",
    "        return {}\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract all replies\n",
    "    all_replies = {}  # tweet_id -> list of replies\n",
    "    \n",
    "    for tid in tweet_ids:\n",
    "        all_replies[tid] = []\n",
    "    \n",
    "    try:\n",
    "        instructions = data.get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "        \n",
    "        entries = []\n",
    "        for instruction in instructions:\n",
    "            if instruction.get('type') == 'TimelineAddEntries':\n",
    "                entries = instruction.get('entries', [])\n",
    "                break\n",
    "        \n",
    "        for entry in entries:\n",
    "            entry_id = entry.get('entryId', '')\n",
    "            \n",
    "            if 'tweet' not in entry_id.lower():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                content = entry.get('content', {})\n",
    "                item_content = content.get('itemContent', {})\n",
    "                tweet_results = item_content.get('tweet_results', {})\n",
    "                result = tweet_results.get('result', {})\n",
    "                \n",
    "                if result.get('__typename') == 'TweetWithVisibilityResults':\n",
    "                    result = result.get('tweet', {})\n",
    "                \n",
    "                legacy = result.get('legacy', {})\n",
    "                core = result.get('core', {}).get('user_results', {}).get('result', {})\n",
    "                user_legacy = core.get('legacy', {})\n",
    "                user_core = core.get('core', {})\n",
    "                \n",
    "                # Check if this is a reply to one of our tweets\n",
    "                in_reply_to = legacy.get('in_reply_to_status_id_str', '')\n",
    "                \n",
    "                if in_reply_to in tweet_ids:\n",
    "                    screen_name = user_legacy.get('screen_name') or user_core.get('screen_name', 'Unknown')\n",
    "                    name = user_legacy.get('name') or user_core.get('name', 'Unknown')\n",
    "                    \n",
    "                    all_replies[in_reply_to].append({\n",
    "                        'comment_id': legacy.get('id_str', ''),\n",
    "                        'text': legacy.get('full_text', ''),\n",
    "                        'username': screen_name,\n",
    "                        'name': name,\n",
    "                        'likes': legacy.get('favorite_count', 0),\n",
    "                        'date': legacy.get('created_at', '')\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error: {e}\")\n",
    "    \n",
    "    return all_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23f6bb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Fetching comments for @DSC_KAU...\n",
      "==================================================\n",
      "üîÑ Getting user ID...\n",
      "‚úÖ User ID: 1608405795707752448\n",
      "\n",
      "üîÑ Fetching last 5 tweets...\n",
      "‚úÖ Found 5 tweets\n",
      "\n",
      "üìù Tweet 1: Ÿäÿ≥ÿ± ŸÜÿßÿØŸä ÿπŸÑŸÖ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿ•ÿπŸÑÿßŸÜ ÿπŸÜ ÿßŸÑŸÅÿ±ŸÇ ÿßŸÑŸÅÿßÿ¶ÿ≤ÿ© ŸÅŸä ...\n",
      "   üí¨ Expected replies: 2\n",
      "   ‚úÖ Extracted 2 comments (direct method)\n",
      "\n",
      "üìù Tweet 2: ÿßŸÑÿ™ÿÆÿ∑Ÿäÿ∑üìù! \n",
      "ÿ≠ŸÑŸÇÿ© ÿßŸÑŸàÿµŸÑ ÿ®ŸäŸÜ ÿßŸÑŸÅŸÉÿ±ÿ© ŸàÿßŸÑÿ™ŸÜŸÅŸäÿ∞ üöÄ\n",
      "\n",
      "ŸÜÿßÿØŸä ...\n",
      "   üí¨ Expected replies: 0\n",
      "   ‚úÖ Extracted 0 comments (direct method)\n",
      "\n",
      "üìù Tweet 3: ÿ¨ÿßŸáÿ≤ŸäŸÜ ŸÑŸÖÿ≥ÿßÿ®ŸÇÿ© ŸäŸÇÿØŸÖŸáÿß ŸÜÿßÿØŸä ÿπŸÑŸÖ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™üî•ÿü\n",
      "\n",
      "ŸÜÿπŸÑŸÜ ŸÑŸÉ...\n",
      "   üí¨ Expected replies: 1\n",
      "   ‚úÖ Extracted 0 comments (direct method)\n",
      "\n",
      "üìù Tweet 4: ŸÑÿ£ŸÜ ÿßŸÑÿπŸäŸÜ ÿ™ŸÅŸáŸÖ ÿ£ÿ≥ÿ±ÿπ ŸÖŸÜ ÿßŸÑŸÉŸÑÿßŸÖ üëÄ\n",
      "\n",
      "ÿ¨ÿßŸäŸäŸÜŸÉŸÖ ŸÅŸä Ÿàÿ±ÿ¥ÿ© ÿπ...\n",
      "   üí¨ Expected replies: 2\n",
      "   ‚úÖ Extracted 0 comments (direct method)\n",
      "\n",
      "üìù Tweet 5: ŸÖŸÜ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ•ŸÑŸâ ÿßŸÑÿ±ÿ§Ÿâ ŸÖÿπ Ÿàÿ±ÿ¥ÿ© ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿßŸÑÿßÿ≥ÿ™...\n",
      "   üí¨ Expected replies: 0\n",
      "   ‚úÖ Extracted 0 comments (direct method)\n",
      "\n",
      "==================================================\n",
      "üîÑ Using search method to find additional replies...\n",
      "üîç Searching for all replies to @DSC_KAU...\n",
      "\n",
      "==================================================\n",
      "üìä Final Summary:\n",
      "   Tweet 1: 2 comments (expected: 2)\n",
      "      1. @silic45: @DSC_KAU ŸÖÿ®ÿ±ŸàŸÉü•≥ü•≥ü•≥\n",
      "      2. @11ttz: @DSC_KAU ÿ®ÿßÿ±ŸÉŸàŸÑŸäŸäŸäŸä üï∫üèªüï∫üèªüï∫üèªüï∫üèªüï∫üèª\n",
      "   Tweet 2: 0 comments (expected: 0)\n",
      "   Tweet 3: 0 comments (expected: 1)\n",
      "   Tweet 4: 0 comments (expected: 2)\n",
      "   Tweet 5: 0 comments (expected: 0)\n",
      "\n",
      "==================================================\n",
      "‚úÖ Done! Total comments extracted: 2\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üöÄ Extract Comments from Last 5 Tweets\n",
    "# ==========================================\n",
    "\n",
    "print(f\"üîç Fetching comments for @{DSC_USERNAME}...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Get user ID\n",
    "print(\"üîÑ Getting user ID...\")\n",
    "user_id = get_user_id(DSC_USERNAME)\n",
    "\n",
    "if not user_id:\n",
    "    print(\"‚ùå Could not find user\")\n",
    "else:\n",
    "    print(f\"‚úÖ User ID: {user_id}\")\n",
    "    \n",
    "    # Step 2: Get latest 5 tweets\n",
    "    print(\"\\nüîÑ Fetching last 5 tweets...\")\n",
    "    tweets = get_latest_tweets(user_id, count=5)\n",
    "    \n",
    "    if not tweets:\n",
    "        print(\"‚ùå No tweets found\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found {len(tweets)} tweets\")\n",
    "        \n",
    "        # Get tweet IDs\n",
    "        tweet_ids = [t['id'] for t in tweets]\n",
    "        \n",
    "        # Step 3: First try direct method for each tweet\n",
    "        all_comments = {}\n",
    "        \n",
    "        for i, tweet in enumerate(tweets, 1):\n",
    "            print(f\"\\nüìù Tweet {i}: {tweet['text'][:50]}...\")\n",
    "            print(f\"   üí¨ Expected replies: {tweet['reply_count']}\")\n",
    "            \n",
    "            comments = get_tweet_comments(tweet['id'])\n",
    "            \n",
    "            all_comments[tweet['id']] = {\n",
    "                'tweet_text': tweet['text'],\n",
    "                'comments': comments\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ Extracted {len(comments)} comments (direct method)\")\n",
    "        \n",
    "        # Step 4: Use search method to find additional replies\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üîÑ Using search method to find additional replies...\")\n",
    "        \n",
    "        search_replies = get_all_replies_via_search(DSC_USERNAME, tweet_ids)\n",
    "        \n",
    "        # Merge search results with direct results\n",
    "        for tweet_id, replies in search_replies.items():\n",
    "            if tweet_id in all_comments:\n",
    "                existing_ids = [c['comment_id'] for c in all_comments[tweet_id]['comments']]\n",
    "                \n",
    "                for reply in replies:\n",
    "                    if reply['comment_id'] not in existing_ids:\n",
    "                        all_comments[tweet_id]['comments'].append(reply)\n",
    "                        print(f\"   ‚ûï Added reply from @{reply['username']} to tweet {tweet_id[:10]}...\")\n",
    "\n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"üìä Final Summary:\")\n",
    "        total_comments = 0\n",
    "        for i, tweet in enumerate(tweets, 1):\n",
    "            count = len(all_comments[tweet['id']]['comments'])\n",
    "            total_comments += count\n",
    "            print(f\"   Tweet {i}: {count} comments (expected: {tweet['reply_count']})\")\n",
    "            \n",
    "            for j, comment in enumerate(all_comments[tweet['id']]['comments'][:3], 1):\n",
    "                text_preview = comment['text'][:50] + \"...\" if len(comment['text']) > 50 else comment['text']\n",
    "                print(f\"      {j}. @{comment['username']}: {text_preview}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"‚úÖ Done! Total comments extracted: {total_comments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed2ca7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Trying simple search for all replies to @DSC_KAU...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Response keys: ['cursor', 'result', 'status']\n",
      "Result keys: ['timeline']\n",
      "üìÅ Saved to: debug_search_v2.json\n"
     ]
    }
   ],
   "source": [
    "# Try simple search for replies to DSC_KAU\n",
    "print(\"üîç Trying simple search for all replies to @DSC_KAU...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "search_url = \"https://twitter241.p.rapidapi.com/search-v2\"\n",
    "\n",
    "response = requests.get(\n",
    "    search_url,\n",
    "    headers=HEADERS,\n",
    "    params={\"query\": \"to:DSC_KAU\", \"count\": \"20\", \"type\": \"Latest\"}\n",
    ")\n",
    "\n",
    "print(f\"Status: {response.status_code}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(f\"Response keys: {list(data.keys())}\")\n",
    "    \n",
    "    # Try to extract tweets\n",
    "    if 'result' in data:\n",
    "        print(f\"Result keys: {list(data['result'].keys())}\")\n",
    "    \n",
    "    # Save for analysis\n",
    "    with open('debug_search_v2.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(\"üìÅ Saved to: debug_search_v2.json\")\n",
    "else:\n",
    "    print(f\"Error response: {response.text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f25729",
   "metadata": {},
   "source": [
    "## üíæ Save Comments to JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60129554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 2 comments from 5 tweets to: dsc_comments.json\n"
     ]
    }
   ],
   "source": [
    "def save_comments_to_json(comments_data, filename=\"dsc_comments.json\"):\n",
    "    \"\"\"Save comments to a JSON file\"\"\"\n",
    "    if not comments_data:\n",
    "        print(\"‚ùå No comments to save\")\n",
    "        return\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(comments_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Count total comments\n",
    "    total = sum(len(data['comments']) for data in comments_data.values())\n",
    "    print(f\"‚úÖ Saved {total} comments from {len(comments_data)} tweets to: {filename}\")\n",
    "\n",
    "# Save the comments\n",
    "if 'all_comments' in dir() and all_comments:\n",
    "    save_comments_to_json(all_comments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
